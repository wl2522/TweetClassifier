{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T01:23:22.912757",
     "start_time": "2017-09-25T01:23:22.906751"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from transformer import *\n",
    "from classify import *\n",
    "from analyze import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T22:13:17.036528",
     "start_time": "2017-09-24T22:13:17.000493"
    },
    "collapsed": true,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#Evaluate unigram models using Naive Bayes, Logistic Regression, and SVM classifiers\n",
    "\n",
    "def unigrams(X, y):\n",
    "    uni_vect = CountVectorizer(encoding='utf-8', stop_words='english', ngram_range=(1, 1),\n",
    "                               decode_error='ignore', strip_accents='ascii')\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "#Apply the remove_urls function to each tweet in the dataset\n",
    "    \n",
    "    X = np.vectorize(remove_urls)(X)\n",
    "    \n",
    "#Create a list of estimators and their associated parameter grids to search through\n",
    "    \n",
    "    model_list = [(('clf', MultinomialNB()),\n",
    "                    {'clf__alpha': np.arange(0.25, 2.0, 0.25)}), \n",
    "                  (('clf', LogisticRegression(random_state=15)),\n",
    "                    {'clf__penalty': ['l1', 'l2'],\n",
    "                    'clf__C': np.arange(0.25, 2.0, 0.25)}),\n",
    "                   (('clf', LinearSVC(random_state=15)),\n",
    "                    {'clf__C': np.arange(0.25, 2.0, 0.25)})\n",
    "                  ]\n",
    "    unigram_results = list()\n",
    "\n",
    "#Vectorize the dataset into unigrams\n",
    "#Perform a 5-fold stratified cross-validation grid search to test each model and find the best parameter values\n",
    "#Append the mean test score and model used to a results list\n",
    "\n",
    "    \n",
    "    for model in model_list:\n",
    "        uni_pipe = Pipeline(steps=[('vect', uni_vect), model[0]])\n",
    "        uni_model = GridSearchCV(uni_pipe, param_grid=model[1], scoring='accuracy', cv=skf, n_jobs=10)\n",
    "        uni_model.fit(X, y)\n",
    "        unigram_results.append((uni_model.best_score_, uni_model))\n",
    "\n",
    "#Sort the results list so that the best performing model is first\n",
    "#Perform a train-test split on the entire training dataset\n",
    "#Refit the best performing model on the new training set and get predictions using the new test set\n",
    "\n",
    "    \n",
    "    unigram_results.sort(reverse=True)\n",
    "    best_uni_model = unigram_results[0][1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=15)\n",
    "    best_uni_model.fit(X_train, y_train)\n",
    "    uni_pred = best_uni_model.predict(X_test)\n",
    "    \n",
    "#Find the 20 largest F-values and their associated feature names and column indices\n",
    "    \n",
    "    uni_features = SelectKBest(score_func=f_classif, k=20)\n",
    "    uni_features.fit(uni_vect.fit_transform(X_train), y_train)\n",
    "    uni_vect.get_feature_names()\n",
    "    top_20 = np.argpartition(uni_features.scores_, -20)[-20:]\n",
    "    \n",
    "    features = list(zip(uni_features.scores_[top_20],\n",
    "                        np.array(uni_vect.get_feature_names())[top_20]))\n",
    "    features.sort(reverse=True)\n",
    "    \n",
    "#Return the average test accuracy of the best performing model, the top 20 features with their weights, and the confusion matrix\n",
    "    \n",
    "    return (unigram_results[0][0], features, confusion_matrix(y_test, uni_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T22:18:25.470075",
     "start_time": "2017-09-24T22:18:25.433036"
    },
    "collapsed": true,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#Evaluate bigram models using Naive Bayes, Logistic Regression, and SVM classifiers\n",
    "\n",
    "def bigrams(X, y):\n",
    "    bi_vect = CountVectorizer(encoding='utf-8', stop_words='english', ngram_range=(2, 2),\n",
    "                              decode_error='ignore', strip_accents='ascii')\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "#Apply the remove_urls function to each tweet in the dataset\n",
    "    \n",
    "    X = np.vectorize(remove_urls)(X)\n",
    "    \n",
    "#Create a list of estimators and their associated parameter grids to search through\n",
    "    \n",
    "    model_list = [(('clf', MultinomialNB()),\n",
    "                    {'clf__alpha': np.arange(0.25, 2.0, 0.25)}), \n",
    "                  (('clf', LogisticRegression(random_state=15)),\n",
    "                    {'clf__penalty': ['l1', 'l2'],\n",
    "                    'clf__C': np.arange(0.25, 2.0, 0.25)}),\n",
    "                   (('clf', LinearSVC(random_state=15)),\n",
    "                    {'clf__C': np.arange(0.25, 2.0, 0.25)})\n",
    "                  ]\n",
    "    bigram_results = list()\n",
    "\n",
    "#Vectorize the dataset into bigrams\n",
    "#Perform a 5-fold stratified cross-validation grid search to test each model and find the best parameter values\n",
    "#Append the mean test score and model used to a results list\n",
    "\n",
    "    \n",
    "    for model in model_list:\n",
    "        bi_pipe = Pipeline(steps=[('vect', bi_vect), model[0]])\n",
    "        bi_model = GridSearchCV(bi_pipe, param_grid=model[1], scoring='accuracy', cv=skf, n_jobs=10)\n",
    "        bi_model.fit(X, y)\n",
    "        bigram_results.append((bi_model.best_score_, bi_model))\n",
    "\n",
    "#Sort the results list so that the best performing model is first\n",
    "#Perform a train-test split on the entire training dataset\n",
    "#Refit the best performing model on the new training set and get predictions using the new test set\n",
    "\n",
    "    \n",
    "    bigram_results.sort(reverse=True)\n",
    "    best_bi_model = bigram_results[0][1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=15)\n",
    "    best_bi_model.fit(X_train, y_train)\n",
    "    bi_pred = best_bi_model.predict(X_test)\n",
    "    \n",
    "#Find the 20 largest F-values and their associated feature names and column indices\n",
    "    \n",
    "    bi_features = SelectKBest(score_func=f_classif, k=20)\n",
    "    bi_features.fit(bi_vect.fit_transform(X_train), y_train)\n",
    "    bi_vect.get_feature_names()\n",
    "    top_20 = np.argpartition(bi_features.scores_, -20)[-20:]\n",
    "    \n",
    "    features = list(zip(bi_features.scores_[top_20],\n",
    "                        np.array(bi_vect.get_feature_names())[top_20]))\n",
    "    features.sort(reverse=True)\n",
    "\n",
    "#Return the average test accuracy of the best performing model, the top 20 features and their weights, and the confusion matrix\n",
    "\n",
    "    return (bigram_results[0][0], features, confusion_matrix(y_test, bi_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T22:21:50.927605",
     "start_time": "2017-09-24T22:21:50.891567"
    },
    "collapsed": true,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#Evaluate trigram models using Naive Bayes, Logistic Regression, and SVM classifiers\n",
    "\n",
    "def trigrams(X, y):\n",
    "    tri_vect = CountVectorizer(encoding='utf-8', stop_words='english', ngram_range=(3, 3),\n",
    "                               decode_error='ignore', strip_accents='ascii')\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "#Apply the remove_urls function to each tweet in the dataset\n",
    "    \n",
    "    X = np.vectorize(remove_urls)(X)\n",
    "    \n",
    "#Create a list of estimators and their associated parameter grids to search through\n",
    "    \n",
    "    model_list = [(('clf', MultinomialNB()),\n",
    "                    {'clf__alpha': np.arange(0.25, 2.0, 0.25)}), \n",
    "                  (('clf', LogisticRegression(random_state=15)),\n",
    "                    {'clf__penalty': ['l1', 'l2'],\n",
    "                    'clf__C': np.arange(0.25, 2.0, 0.25)}),\n",
    "                   (('clf', LinearSVC(random_state=15)),\n",
    "                    {'clf__C': np.arange(0.25, 2.0, 0.25)})\n",
    "                  ]\n",
    "    trigram_results = list()\n",
    "\n",
    "#Vectorize the dataset into trigrams\n",
    "#Perform a 5-fold stratified cross-validation grid search to test each model and find the best parameter values\n",
    "#Append the mean test score and model used to a results list\n",
    "\n",
    "    \n",
    "    for model in model_list:\n",
    "        tri_pipe = Pipeline(steps=[('vect', tri_vect), model[0]])\n",
    "        tri_model = GridSearchCV(tri_pipe, param_grid=model[1], scoring='accuracy', cv=skf, n_jobs=10)\n",
    "        tri_model.fit(X, y)\n",
    "        trigram_results.append((tri_model.best_score_, tri_model))\n",
    "\n",
    "#Sort the results list so that the best performing model is first\n",
    "#Perform a train-test split on the entire training dataset\n",
    "#Refit the best performing model on the new training set and get predictions using the new test set\n",
    "\n",
    "    \n",
    "    trigram_results.sort(reverse=True)\n",
    "    best_tri_model = trigram_results[0][1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=15)\n",
    "    best_tri_model.fit(X_train, y_train)\n",
    "    tri_pred = best_tri_model.predict(X_test)\n",
    "    \n",
    "#Find the 20 largest F-values and their associated feature names and column indices\n",
    "    \n",
    "    tri_features = SelectKBest(score_func=f_classif, k=20)\n",
    "    tri_features.fit(tri_vect.fit_transform(X_train), y_train)\n",
    "    tri_vect.get_feature_names()\n",
    "    top_20 = np.argpartition(tri_features.scores_, -20)[-20:]\n",
    "    \n",
    "    features = list(zip(tri_features.scores_[top_20],\n",
    "                        np.array(tri_vect.get_feature_names())[top_20]))\n",
    "    features.sort(reverse=True)\n",
    "\n",
    "#Return the average test accuracy of the best performing model, the top 20 features and their weights, and the confusion matrix\n",
    "\n",
    "    return (trigram_results[0][0], features, confusion_matrix(y_test, tri_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T01:23:26.883652",
     "start_time": "2017-09-25T01:23:26.787557"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_load('train_newline.txt', 'dev_newline.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T22:15:24.969990",
     "start_time": "2017-09-24T22:13:27.030359"
    },
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best unigram model accuracy:  0.711275\n",
      "Top 20 unigram features: \n",
      "\n",
      "(270.80972172840893, 'tcot')\n",
      "(116.16442477018541, 'uniteblue')\n",
      "(108.67366319808505, 'p2')\n",
      "(83.867941299774074, 'dailykos')\n",
      "(80.539265072797747, 'voteblue')\n",
      "(77.203170675642397, 'republican')\n",
      "(50.593496673136855, 'roc')\n",
      "(49.822341989974547, 'alpolitics')\n",
      "(49.33974062122131, 'teaparty')\n",
      "(46.710506721754761, 'sayfie')\n",
      "(46.201135432064966, 'dandc')\n",
      "(44.345668087407731, 'pjnet')\n",
      "(43.940708250175369, 'gapol')\n",
      "(40.137570950793211, 'obama')\n",
      "(38.244388776805863, 'utpol')\n",
      "(34.017490902352684, 'ccot')\n",
      "(31.872043105193164, 'equality')\n",
      "(31.586163986676247, 'gop2012')\n",
      "(30.733573967456909, 'crnc')\n",
      "(30.691082840554412, 'jjauthor')\n",
      "Unigram model confusion matrix: \n",
      " [[3439 1446]\n",
      " [1479 3636]]\n"
     ]
    }
   ],
   "source": [
    "uni_accuracy, uni_features, uni_matrix = unigrams(X_train, y_train)\n",
    "\n",
    "print('Best unigram model accuracy: ', uni_accuracy)\n",
    "print('Top 20 unigram features: \\n')\n",
    "\n",
    "for feature in uni_features:\n",
    "    print(feature)\n",
    "    \n",
    "print('Unigram model confusion matrix: \\n', uni_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T22:20:37.001639",
     "start_time": "2017-09-24T22:18:41.493858"
    },
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best bigram model accuracy:  0.6426\n",
      "Top 20 bigram features: \n",
      "\n",
      "(67.724711443326996, 'http dailykos')\n",
      "(50.340675529443885, 'http tcot')\n",
      "(32.545446096233405, 'sayfie http')\n",
      "(26.216555715034289, 'http dandc')\n",
      "(23.698454048968582, 'scott brown')\n",
      "(23.660902802759356, 'tcot tlot')\n",
      "(22.000230237364836, 'rt foxnews')\n",
      "(21.109527304664933, 'rt gop')\n",
      "(20.966079621025784, 'parikh daily')\n",
      "(20.966079621025784, 'gaurav parikh')\n",
      "(20.378959988369964, 'http stories')\n",
      "(20.187511405332319, 'p2 uniteblue')\n",
      "(20.08454490634993, 'county republican')\n",
      "(19.126889758983726, 'tlot tcot')\n",
      "(18.866893510042352, 'ritnews roc')\n",
      "(18.588615311321057, 'gop http')\n",
      "(18.173859134170382, 'rt thedemocrats')\n",
      "(18.099539371888721, 'voteblue http')\n",
      "(18.099539371888721, 'http politicususa')\n",
      "(17.831526141171217, 'http http')\n",
      "Bigram model confusion matrix: \n",
      " [[2367 2518]\n",
      " [1108 4007]]\n"
     ]
    }
   ],
   "source": [
    "bi_accuracy, bi_features, bi_matrix = bigrams(X_train, y_train)\n",
    "\n",
    "print('Best bigram model accuracy: ', bi_accuracy)\n",
    "print('Top 20 bigram features: \\n')\n",
    "\n",
    "for feature in bi_features:\n",
    "    print(feature)\n",
    "    \n",
    "print('Bigram model confusion matrix: \\n', bi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T22:24:26.539994",
     "start_time": "2017-09-24T22:22:36.679756"
    },
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trigram model accuracy:  0.5603\n",
      "Top 20 trigram features: \n",
      "\n",
      "(20.966079621025784, 'parikh daily http')\n",
      "(20.966079621025784, 'gaurav parikh daily')\n",
      "(17.211954378180863, 'http twurl nl')\n",
      "(15.297518719554629, 'http gop2012 tcot')\n",
      "(14.670241697400172, 'tennessee democrat http')\n",
      "(14.670241697400172, 'inbox constantcontact http')\n",
      "(14.670241697400172, 'http ritnews roc')\n",
      "(14.670241697400172, 'democrat http stories')\n",
      "(14.340488224933132, 'tru town films')\n",
      "(14.340488224933132, 'gop2012 tcot tlot')\n",
      "(14.340488224933132, 'bunker coyotered9 http')\n",
      "(14.282232704066855, 'just posted photo')\n",
      "(13.621436991509031, 'democratic underground http')\n",
      "(13.621436991509031, 'ctl p2 uniteblue')\n",
      "(11.524257270968477, 'tlot ctl p2')\n",
      "(11.470145786590141, 'county republican party')\n",
      "(10.513614574229878, 'tcot tlot gop')\n",
      "(10.513614574229878, 'north carolina investigating')\n",
      "(10.513614574229878, 'investigating kay hagan')\n",
      "(10.475882197642132, 'weekend li blue')\n",
      "Trigram model confusion matrix: \n",
      " [[ 712 4173]\n",
      " [ 270 4845]]\n"
     ]
    }
   ],
   "source": [
    "tri_accuracy, tri_features, tri_matrix = trigrams(X_train, y_train)\n",
    "\n",
    "print('Best trigram model accuracy: ', tri_accuracy)\n",
    "print('Top 20 trigram features: \\n')\n",
    "\n",
    "for feature in tri_features:\n",
    "    print(feature)\n",
    "    \n",
    "print('Trigram model confusion matrix: \\n', tri_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T23:51:45.062178",
     "start_time": "2017-09-24T23:51:45.009125"
    },
    "collapsed": true,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#Find the best model parameters using Naive Bayes, Logistic Regression, and SVM models\n",
    "\n",
    "def find_best_model(X, y):\n",
    "    n_vect = CountVectorizer(encoding='utf-8', stop_words='english', decode_error='ignore',\n",
    "                             strip_accents='ascii')\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    \n",
    "#Apply the remove_urls function to each tweet in the dataset\n",
    "    \n",
    "    X = np.vectorize(remove_urls)(X)\n",
    "    \n",
    "#Combine the array of vectorized text with an array containing additional writing style/content features\n",
    "    \n",
    "    union = FeatureUnion([('n_vect', n_vect), ('style_features', FeatureExtractor())])\n",
    "    \n",
    "#Create a list of estimators and their associated parameter grids to search through\n",
    "#Search the same parameter grids over different n-gram combinations in case they improve performance\n",
    "\n",
    "    \n",
    "    model_list = [(('clf', MultinomialNB()),\n",
    "                    {'clf__alpha': np.arange(0.25, 2.0, 0.25),\n",
    "                     'union__n_vect__ngram_range': [(1, 1), (2, 2), (3, 3), (1, 2), (1, 3), (2, 3)],\n",
    "                    }),\n",
    "                  (('clf', LogisticRegression(random_state=15)),\n",
    "                    {'clf__penalty': ['l1', 'l2'],\n",
    "                    'clf__C': np.arange(0.25, 2.0, 0.25),\n",
    "                    'union__n_vect__ngram_range': [(1, 1), (2, 2), (3, 3), (1, 2), (1, 3), (2, 3)]                   \n",
    "                    }), \n",
    "                   (('clf', LinearSVC(random_state=15)),\n",
    "                    {'clf__C': np.arange(0.25, 2.0, 0.25),\n",
    "                     'union__n_vect__ngram_range': [(1, 1), (2, 2), (3, 3), (1, 2), (1, 3), (2, 3)]\n",
    "                    })\n",
    "                  ]\n",
    "    \n",
    "    ngram_results = list()\n",
    "\n",
    "#Vectorize the dataset into unigrams\n",
    "#Perform a 5-fold stratified cross-validation grid search to test each model and find the best parameter values\n",
    "#Append each model and its mean test score to a list\n",
    "\n",
    "    \n",
    "    for model in model_list:\n",
    "        n_pipe = Pipeline(steps=[('union', union),\n",
    "                                 model[0]])\n",
    "        n_model = GridSearchCV(n_pipe, param_grid=model[1], scoring='accuracy', cv=skf,\n",
    "                               verbose=1, n_jobs=10)\n",
    "        n_model.fit(X, y)\n",
    "        ngram_results.append((n_model.best_score_, n_model))\n",
    "\n",
    "#Sort the results list so that the best performing model is first\n",
    "#Perform a train-test split on the entire training dataset\n",
    "#Refit the best performing model on the new training set and get predictions using the new test set\n",
    "\n",
    "    \n",
    "    ngram_results.sort(reverse=True)\n",
    "    best_model = ngram_results[0][1].best_estimator_\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=15)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    best_pred = best_model.predict(X_test)\n",
    "    \n",
    "#Find the 20 largest F-values and their associated feature names and column indices\n",
    "    \n",
    "    n_features = SelectKBest(score_func=f_classif, k=20)\n",
    "    n_features.fit(n_vect.fit_transform(X_train), y_train)\n",
    "    top_20 = np.argpartition(n_features.scores_, -20)[-20:]\n",
    "    \n",
    "    features = list(zip(n_features.scores_[top_20],\n",
    "                        np.array(n_vect.get_feature_names())[top_20]))\n",
    "    features.sort(reverse=True)\n",
    "\n",
    "#Print the best model and its associated optimal parameters    \n",
    "#Return the average test accuracy of the best performing model, the top 20 features and their weights, and the confusion matrix\n",
    "    \n",
    "    return (ngram_results[0][0], features, confusion_matrix(y_test, best_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T00:56:56.703569",
     "start_time": "2017-09-25T00:27:05.238362"
    },
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   30.0s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=10)]: Done 210 out of 210 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done 420 out of 420 | elapsed: 17.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=10)]: Done 210 out of 210 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n-gram model accuracy:  0.7185\n",
      "Top 20 n-gram features: \n",
      "\n",
      "(270.80972172840893, 'tcot')\n",
      "(116.16442477018541, 'uniteblue')\n",
      "(108.67366319808505, 'p2')\n",
      "(83.867941299774074, 'dailykos')\n",
      "(80.539265072797747, 'voteblue')\n",
      "(77.203170675642397, 'republican')\n",
      "(50.593496673136855, 'roc')\n",
      "(49.822341989974547, 'alpolitics')\n",
      "(49.33974062122131, 'teaparty')\n",
      "(46.710506721754761, 'sayfie')\n",
      "(46.201135432064966, 'dandc')\n",
      "(44.345668087407731, 'pjnet')\n",
      "(43.940708250175369, 'gapol')\n",
      "(40.137570950793211, 'obama')\n",
      "(38.244388776805863, 'utpol')\n",
      "(34.017490902352684, 'ccot')\n",
      "(31.872043105193164, 'equality')\n",
      "(31.586163986676247, 'gop2012')\n",
      "(30.733573967456909, 'crnc')\n",
      "(30.691082840554412, 'jjauthor')\n",
      "Best n-gram model confusion matrix: \n",
      " [[3396 1489]\n",
      " [1389 3726]]\n"
     ]
    }
   ],
   "source": [
    "best_accuracy, best_features, best_matrix = find_best_model(X_train, y_train)\n",
    "\n",
    "print('Best n-gram model accuracy: ', best_accuracy)\n",
    "print('Top 20 n-gram features: \\n')\n",
    "\n",
    "for feature in best_features:\n",
    "    print(feature)\n",
    "    \n",
    "print('Best n-gram model confusion matrix: \\n', best_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T01:06:59.555681",
     "start_time": "2017-09-25T01:06:38.500485"
    },
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model test accuracy:  0.628\n",
      "Top 20 n-gram features: \n",
      "\n",
      "(374.86902465625639, 'tcot')\n",
      "(167.10234220909032, 'uniteblue')\n",
      "(135.98224927300524, 'p2')\n",
      "(112.66250893302396, 'voteblue')\n",
      "(108.31607072289431, 'dailykos')\n",
      "(90.062102847283427, 'http dailykos')\n",
      "(85.934199847461727, 'republican')\n",
      "(74.210790555055183, 'http tcot')\n",
      "(71.757927421002378, 'roc')\n",
      "(71.614476461460924, 'teaparty')\n",
      "(66.095208185839184, 'alpolitics')\n",
      "(63.009015091627745, 'dandc')\n",
      "(58.548525727153155, 'sayfie')\n",
      "(56.855083438833759, 'pjnet')\n",
      "(50.059388638209946, 'gapol')\n",
      "(48.209497373200705, 'obama')\n",
      "(46.161888184233831, 'utpol')\n",
      "(41.900431792274219, 'ccot')\n",
      "(41.254243545006346, 'crnc')\n",
      "(40.918250392611483, 'victory')\n",
      "Best n-gram model confusion matrix: \n",
      " [[1651 1020]\n",
      " [ 840 1489]]\n"
     ]
    }
   ],
   "source": [
    "print('Best model test accuracy: ', best_model('train_newline.txt', 'dev_newline.txt'))\n",
    "top_features, conf_matrix = contingency_matrix('model.pkl', 'test.pkl')\n",
    "\n",
    "print('Top 20 n-gram features: \\n')\n",
    "\n",
    "for feature in top_features:\n",
    "    print(feature)\n",
    "\n",
    "print('Best n-gram model confusion matrix: \\n', conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
